{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions, Explanations and Functions Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under the ROC Curve (AUC)\n",
    "* [Link](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) Definition: AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).\n",
    "* FP Rate is the X axis TP Rate is the y axis.\n",
    "* AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n",
    "* AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\n",
    "* AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Squared Distribution\n",
    "* \"It is a way of taking the difference between the actual and expected value and translating that into a number.\" [Link](https://www.khanacademy.org/math/ap-statistics/chi-square-tests/chi-square-goodness-fit/v/chi-square-statistic) \"We can use it to determine what is the probability of getting a result this extreme or more extreme. If it is lower than our significance level we reject the null hypothesis and it suggests the alternative.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation:\n",
    "* Correlation Matrix = $\\mathbf{R}$\n",
    "* Provides the direction and strength of a relationship.\n",
    "* the correlation result will always be between -1 and +1 and its scale is independent of the scale of the variables themselves.\n",
    "* Correlation is standardized (think z-score) It is the standardized version of the correlatin matrix. This means you would want to use this if your variables are being measured using different scales.\n",
    "* Correlation is only applicable to LINEAR relationships. There are many other tyes of relationships that can exist between two variables.\n",
    "* Correlation is NOT Causation.\n",
    "* Correlation strength does not necessarily mena the correlation is statistically significant; related to sample size.\n",
    "* Interpretation: Positive relationship is near +1, negative relationship is near -1, no correlation it will be near 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance:\n",
    "* [Definition](http://mathworld.wolfram.com/Covariance.html) - Covariance provides a measure of the strength of the correlation between two or more sets of random variates. Other sources say it will just so you the direction it will not show you the strength.\n",
    "* Lecture Slides - Variance-Covariance Matrix, S - Measures variability in the variables\n",
    "* Covariance is one of a family of statistical measures used to analyze the linear relationship between two variables. It is a descriptive measure of the linear association between two variables.\n",
    "* Covariance result has no upper or lower bound and its size is dependent on the scale of the variables.\n",
    "* Covariance is not standardized.\n",
    "\n",
    "## Covariance Matrix:\n",
    "* Variance-covariance matrix = $\\mathbf{S}$\n",
    "* Variance-covariance matrix aka the covariance matrix or the dispersion matrix.\n",
    "* [Covariance Matrix](https://www.youtube.com/watch?v=locZabK4Als)\n",
    "* Interpretation: A positive value indicates a direct or increasing linear relationship, A negative value indicates a decreasing relationship. Covariance at or around zero indicates that there is not a linear relationship between the two. Covariance does not tell us anything about the strength of the relationship just the direction of the relationship. To find the strength of the relationship you will want to look at correlation.\n",
    "* The diagonal of a covariance matrix provides the variance of each individual variable; covariance with itself.\n",
    "* The off-diagonal entries in the matrix provide the covariance between each variable pair.\n",
    "* Remember that the standard deviation is simply the square root of the variance so that can be calculated as well.\n",
    "* [Explanation](https://datascienceplus.com/understanding-the-covariance-matrix/) of the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues\n",
    "* For a square matrix $\\mathbf{A}$ and a non-zero vector $\\mathbf{x}$ if $\\mathbf{Ax = \\lambda x}$ for some constant $\\lambda{}$ then we say that $\\lambda$ is an eigenvalue of $\\mathbf{A}$ and that $\\mathbf{x}$ is an eigenvecotr of $\\mathbf{A}$ corresponding to the eigenvalue of $\\lambda$.\n",
    "* The scalar that is used to transform (stretch) an Eigenvector\n",
    "* The Eigenvalue divided by the total variance will give you the proportion of the variability explained by that vector?  ASK ABOUT THAT INTERPRETATION!!!!!\n",
    "* The total variance comes from adding up all the diagonal values in the covariance matrix.\n",
    "* At least for the correlation matrix you can find out the proportion of variability that is explained by a variable through its eigenvalue. By adding up all of the eigenvalues and then dividing the eigenvalue by the sum of all eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors\n",
    "* It s a vector that is scaled up by a transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogeneity (Homogeneous)\n",
    "A data set is homogeneous if it is made up of things (i.e. people, cells or traits) that are similar to each other. For example a data set made up of 20-year-old college students enrolled in Physics 101 is a homogeneous sample. [Link](https://www.statisticshowto.datasciencecentral.com/homogeneity-homogeneous/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Matrix\n",
    "* [Description](http://mathworld.wolfram.com/IdentityMatrix.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Matrix\n",
    "* [Definition of Orthogonal](http://mathworld.wolfram.com/Orthogonal.html)\n",
    "* [Description of Orthogonal Matrix](http://mathworld.wolfram.com/OrthogonalMatrix.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic Curve (ROC)\n",
    "* [Link](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) Definition: An ROC curve is a graph showing the performance of a classification model at all classification thresholds. The Curve plots two parameters:\n",
    "    * True Positive Rate (TPR) synonym for recall and is therfore defined as follows. TPR = TP/(TP + FN)\n",
    "    * False Positive Rate (FPR) is defined as follows: FPR = FP/(FP + TN)   \n",
    "    \n",
    "An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewness\n",
    "This is one way to insert an image.  \n",
    "![Skewness](images/images.png)\n",
    "\n",
    "\n",
    "\n",
    "To be able to insert an image and resize it use this code.  \n",
    "<img src=\"images/images.png\" style=\"width:100%;height:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Eiganvalues & Eiganvectors](https://medium.com/fintechexplained/what-are-eigenvalues-and-eigenvectors-a-must-know-concept-for-machine-learning-80d0fd330e47)\n",
    "* [Eigenvectos & Eigenvalues Video](https://www.youtube.com/watch?v=PFDu9oVAE-g)\n",
    "* [Simple Statistics](https://www.youtube.com/watch?v=xGbpuFNR1ME&list=PLIeGtxpvyG-JMH5fGDWhtniyET88Mexcw&index=5)\n",
    "* [Symbols, Meaning, Latex](https://en.wikipedia.org/wiki/List_of_mathematical_symbols)\n",
    "* [PCA Eigenvectors and Eigenvaleus](https://towardsdatascience.com/pca-eigenvectors-and-eigenvalues-1f968bc6777a)\n",
    "* [PCA Analysis: Minitab](https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/principal-components/interpret-the-results/key-results/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><a href=\"http://mathworld.wolfram.com/Covariance.html\">Wolfram</li>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "  <li>Coffee</li>\n",
    "  <li>Tea</li>\n",
    "  <li>Milk</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
